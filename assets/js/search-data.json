{
  
    
        "post0": {
            "title": "Intuitive Explanation of Straight-Through Estimators with PyTorch Implementation",
            "content": "Sometimes we want to put a threshold function at the output of a layer. This can be for a variety of reasons. One of them is that we want to summarize the activations into binary values. This binarization of activations can be useful in autoencoders. . However, thresholding poses a problem during backpropagation. The derivative of threshold functions is zero. This lack of gradient results in our network not learning anything. To solve this problem we use straight-through estimators (STE). . What is a Straight-Through Estimator? . Lets suppose we want to binarize the activations of a layer using the following function: . f(x)={1,x&gt;00,x≤0f(x) = begin{cases} 1, &amp; x &gt; 0 0, &amp; x le 0 end{cases}f(x)={1,0,​x&gt;0x≤0​ . This function will return 1 for every value that is greater than 0 otherwise it will return 0. . As mentioned earlier, the problem with this function is that its gradient is zero. To overcome this issue we will use a straight-through estimator in the backward pass. . A straight-through estimator is exactly what it sounds like. It estimates the gradients of a function. Specifically it ignores the derivative of the threshold function and passes on the incoming gradient as if the function was an identity function. The following diagram will help explain it better. . . You can see how the threshold function is bypassed in the backward pass. That’s it, this is what a straight-through estimator does. It makes the gradient of the threshold function look like the gradient of the identity function. . Implementation in PyTorch . As of right now, PyTorch doesn’t include an implementation of an STE in its APIs. So, we will have to implement it ourselves. To do this we will need to create a Function class and a Module class. The Function class will contain the forward and backward functionality of the STE. The Module class is where the STE Function object will be created and used. We will use the STE Module in our neural networks. . Below is the implementation of the STE Function class: . class STEFunction(torch.autograd.Function): @staticmethod def forward(ctx, input): return (input &gt; 0).float() @staticmethod def backward(ctx, grad_output): return F.hardtanh(grad_output) . PyTorch lets us define custom autograd functions with forward and backward functionality. Here we have defined an autograd function for a straight-through estimator. In the forward pass we want to convert all the values in the input tensor from floating point to binary. In the backward pass we want to pass the incoming gradients without modifying them. This is to mimic the identity function. Although, here we are performing the F.hardtanh operation on the incoming gradients. This operation will clamp the gradient between -1 and 1. We are doing this so that the gradients do not get too big. . Now, lets implement the STE Module class: . class StraightThroughEstimator(nn.Module): def __init__(self): super(StraightThroughEstimator, self).__init__() def forward(self, x): x = STEFunction.apply(x) return x . You can see that we have used the STE Function class we defined in the forward function. To use autograd functions we have to pass the input to the apply method. Now, we can use this module in our neural networks. . A common way to use STE is inside the bottleneck layer of autoencoders. Here is an implementation of such an autoencoder: . class Autoencoder(nn.Module): def __init__(self): super(Autoencoder, self).__init__() self.encoder = nn.Sequential( nn.Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)), nn.ReLU(), nn.Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)), nn.BatchNorm2d(128), nn.ReLU(), nn.Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)), nn.BatchNorm2d(256), nn.ReLU(), nn.Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)), nn.BatchNorm2d(512), nn.ReLU(), StraightThroughEstimator(), ) self.decoder = nn.Sequential( nn.ConvTranspose2d(512, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1)), nn.BatchNorm2d(256), nn.ReLU(), nn.ConvTranspose2d(256, 128, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1)), nn.BatchNorm2d(128), nn.ReLU(), nn.ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)), nn.BatchNorm2d(64), nn.ReLU(), nn.ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)), nn.Tanh(), ) def forward(self, x, encode=False, decode=False): if encode: x = self.encoder(x) elif decode: x = self.decoder(x) else: encoding = self.encoder(x) x = self.decoder(encoding) return x . This autoencoder is made for the MNIST dataset. It will compress the 28x28 image into a 1x1 image with 512 channels. Then decode it back to 28x28 image. . I’ve placed the STE at the end of the encoder. It will convert all of the values of the tensor it receives to binary. You might have noticed I’ve used an unconventional forward function. I’ve added two new arguments encode and decode which are either True or False. If encode is set to True, the network will return the output of the encoder. Similarly if decode is set to True, the network expects a valid encoding and it will decode it back to an image. . I trained the autoencoder for 5 epochs on the MNIST dataset with MSE loss. Here are the reconstructions on the test set: . . As you can see, the reconstructions are pretty good. STEs can be used in neural networks without much loss in performance. . Full Code . import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim import torch.autograd as autograd from torchvision import datasets, transforms import numpy as np import matplotlib.pyplot as plt from tqdm import tqdm device = &#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39; # dataset preparation transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.5, ), (0.5, )) ]) trainset = datasets.MNIST(&#39;dataset/&#39;, train=True, download=True, transform=transform) testset = datasets.MNIST(&#39;dataset/&#39;, train=False, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True) # defining networks class STEFunction(autograd.Function): @staticmethod def forward(ctx, input): return (input &gt; 0).float() @staticmethod def backward(ctx, grad_output): return F.hardtanh(grad_output) class StraightThroughEstimator(nn.Module): def __init__(self): super(StraightThroughEstimator, self).__init__() def forward(self, x): x = STEFunction.apply(x) return x class Autoencoder(nn.Module): def __init__(self): super(Autoencoder, self).__init__() self.encoder = nn.Sequential( nn.Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)), nn.ReLU(), nn.Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)), nn.BatchNorm2d(128), nn.ReLU(), nn.Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)), nn.BatchNorm2d(256), nn.ReLU(), nn.Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)), nn.BatchNorm2d(512), nn.ReLU(), StraightThroughEstimator(), ) self.decoder = nn.Sequential( nn.ConvTranspose2d(512, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1)), nn.BatchNorm2d(256), nn.ReLU(), nn.ConvTranspose2d(256, 128, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1)), nn.BatchNorm2d(128), nn.ReLU(), nn.ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)), nn.BatchNorm2d(64), nn.ReLU(), nn.ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)), nn.Tanh(), ) def forward(self, x, encode=False, decode=False): if encode: x = self.encoder(x) elif decode: x = self.decoder(x) else: encoding = self.encoder(x) x = self.decoder(encoding) return x net = Autoencoder().to(device) optimizer = optim.Adam(net.parameters(), lr=0.001, betas=(0.5, 0.999)) criterion_MSE = nn.MSELoss().to(device) # train loop epoch = 5 for e in range(epoch): print(f&#39;Starting epoch {e} of {epoch}&#39;) for X, y in tqdm(trainloader): optimizer.zero_grad() X = X.to(device) reconstruction = net(X) loss = criterion_MSE(reconstruction, X) loss.backward() optimizer.step() print(f&#39;Loss: {loss.item()}&#39;) # test loop i = 1 fig = plt.figure(figsize=(10, 10)) for X, y in testloader: X_in = X.to(device) recon = net(X_in).detach().cpu().numpy() if i &gt;= 10: break fig.add_subplot(5, 2, i).set_title(&#39;Original&#39;) plt.imshow(X[0].reshape((28, 28)), cmap=&quot;gray&quot;) fig.add_subplot(5, 2, i+1).set_title(&#39;Reconstruction&#39;) plt.imshow(recon[0].reshape((28, 28)), cmap=&quot;gray&quot;) i += 2 fig.tight_layout() plt.show() . I hope you found this post helpful. Thanks for reading! .",
            "url": "https://hassanaskary.com/python/pytorch/deep%20learning/2020/09/19/intuitive-explanation-of-straight-through-estimators.html",
            "relUrl": "/python/pytorch/deep%20learning/2020/09/19/intuitive-explanation-of-straight-through-estimators.html",
            "date": " • Sep 19, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Getting Started with PyTorch",
            "content": "PyTorch is a machine learning framework by Facebook. It allows you to do tensor computation on the GPU, and design, train, and deploy deep learning models. . In this post we’ll cover the following topics: . Installing PyTorch | Getting familiar with commonly used APIs | Building a classifier for MNIST | I will assume familiarity with machine learning and deep learning. . Installing PyTorch . Go to the official website to download and install PyTorch. Scroll down a bit and you’ll see the “Start Locally” section. Select the option according to your OS, CUDA version, and preference of Pip or Conda. . I’m using Pop OS 20.04 right now without GPU. So I’ll select the following: . . Copy and paste the command indicated by “Run this Command” into your terminal and execute. This should install PyTorch on your system. . Getting Familiar with Commonly Used APIs . PyTorch consists of many APIs. I won’t cover individual syntax and commands but give you an overview of what each API does. This way you will have a birds eye view of PyTorch and when you get stuck you will know where to look. . The commonly used APIs/libraries are: . torch torch.nn | torch.nn.functional | torch.optim | torch.utils.data | | torchvision | torch . The torch API provides the tensor data structure and its associated methods and mathematical operations. A tensor is similar to a numpy array. Difference between a numpy array and a torch tensor is that the latter can utilize a GPU to do computations. . torch.nn . The torch.nn API contains the building blocks to design and build a neural network. These building blocks are called modules and they are subclasses that inherit from torch.nn.Module base class. They are stateful meaning they automatically store their weights, gradients and other attributes. They also need to be initialized before usage. . The API contains fully-connected linear layers, convolution layers, activation functions, loss functions, normalization layers, dropout etc. . torch.nn.functional . The torch.nn.functional API contains useful functions like activations and loss functions etc. However, they are not stateful meaning they won’t store their attributes automatically. Instead we will have to store them manually. . These functions can be used for applying layers that don’t need to be learned like activations or pooling etc. . This API provides additional flexibility in designing your models. . torch.optim . The torch.optim API contains optimizers like Adam, SGD, and RMSprop etc. and methods for calculating gradients and applying them. . torch.utils.data . This is a utility that provides the DataLoader class which is used to load data from a folder or from a built-in dataset. . torchvision . The torchvision library contains datasets, models, and utilities related to computer vision. The torchvision.datasets package contains popular datasets like MNIST, ImageNet, and COCO etc. The torchvision.models package contains famous pre-trained or untrained vision models like ResNet, Inception, and Mask R-CNN etc. The torchvision.transforms package contains image transformations used to preprocess datasets like converting images to torch tensor, cropping, or resizing etc. . Building a Classifier for MNIST . There are many ways to build a neural network in PyTorch. In this post I’ll demonstrate building a convolutional neural network. It will have three convolution layers followed by three linear layers with Relu activations and finally a cross entropy loss. . We will be doing things in the following order: . Import packages | Load and preprocess data | Build the network | Write the training loop | Write the testing loop | Import Packages . First we will import the required packages. . import torch import torch.nn as nn import torch.optim as optim import torchvision import torchvision.datasets as datasets import torchvision.transforms as transforms . Load and Preprocess Data . We will use the built-in MNIST digits dataset. But first we have to specify how we will preprocess the images. . preprocess = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.5, ), (0.5, )) ]) . Here, transforms.Compose() is a list of the transformations we want to apply to the images in order. First, we will convert the images to torch tensors then we will normalize them. . The syntax of transforms.Normalize() is as follows: . transforms.Normalize(mean, standard deviation) . Each index in those two tuples correspond to a channel in the image. Since MNIST images are grayscale they have only one channel. So we are setting the mean and standard deviation of that one channel to 0.5 yielding a mean of 0 and standard deviation of 1. . Lets download the dataset. . trainset = datasets.MNIST(&#39;dataset/&#39;, train=True, download=True, transform=preprocess) testset = datasets.MNIST(&#39;dataset/&#39;, train=False, download=True, transform=preprocess) . Here, we are creating the train and test sets. They will be downloaded in “dataset/” folder. The train argument specifies whether to download train or test set. The transform argument takes in the transforms.Compose() object and preprocesses the images according to it. . Now we will create data loaders that will return a batch of data on each iteration. . trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True) . Build the Network . Finally the fun part. First we will define some constants. . nf = 32 lr = 0.0001 beta1 = 0.5 beta2 = 0.999 device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot; . Here, nf is the number of kernels/filters/neurons (from now on I will call them filters). For device we are checking whether a CUDA enabled GPU exists. If it does not exist then we will use the CPU. . Now lets define the network. In PyTorch by convention we define models as a subclass of torch.nn.Module. We initialize layers in __init__() and connect them in forward() function. . class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.convs = nn.Sequential( nn.Conv2d(1, nf, kernel_size=3, stride=1, padding=1), nn.ReLU(), nn.Conv2d(nf, nf, kernel_size=3, stride=1, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2), # resultant size will be 32x14x14 nn.Conv2d(nf, nf, kernel_size=3, stride=1, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2), # resultant size will be 32x7x7 ) self.linears = nn.Sequential( nn.Linear(1568, 100), # width=7, height=7, filters=32; linear layer input = 7*7*32 = 1568 nn.ReLU(), nn.Linear(100, 50), nn.ReLU(), nn.Linear(50, 10), ) def forward(self, x): x = self.convs(x) x = x.view(x.size(0), -1) # flattening, result will be (64, 1568) x = self.linears(x) return x . Ok, a lot to unpack here. In the __init__() you can see self.convs and self.linears being defined. The nn.Sequential is like a list. It contains modules and executes them in order sequentially. . We are defining the convolution part and the fully-connected or linear part separately. All layers will be initialized randomly. . Now, lets look at the individual layers. In self.convs we first see nn.Conv2d() this is a convolution layer. The arguments correspond to: . nn.Conv2d(number of input channels, number of filters, filter size, stride, padding) . This convolution layer is connected to a relu layer. This continues until we reach a max pool layer. Its arguments correspond to: . nn.MaxPool2d(filter size, stride) . Moving on in self.linears we have the fully connected layers. Its arguments correspond to: . nn.Linear(input feature/vector size, number of neurons in layer) . In the first linear layer we see that the input is 1568 and the output or number of neurons is 100. In PyTorch we have to define and initialize our layers before using them. This means we have to specify there input and output sizes. . I have calculated the output size of self.convs. We can calculate the output size of convolution using the following formulas: . outputWidth = (inputWidth - filterSize + (2 * padding)) / stride + 1 outputHeight = (inputHeight - filterSize + (2 * padding)) / stride + 1 . There are also max pool layers which change the size of our features. We can calculate the output size of max pool using: . outputWidth = (inputWidth - filterSize) / stride + 1 outputHeight = (inputHeight - filterSize) / stride + 1 . This calculation can be automated by writing a custom module. But that is out of the scope of this guide. . Moving on to the forward() function. This is where we bring together the self.convs and self.linears and connect them. Here x is the input features/tensors. . After getting output from self.convs we flatten it by using the view() function. The view() function is similar to reshape() in numpy. Here the first argument is set to x.size(0) which is the batch size. The second argument is “-1” which tells PyTorch to infer the remaining size which in this case will be equal to 1568. After this the tensor will be flattened and ready to be passed to self.linears. Finally, we will return the result. . Write the Training Loop . Lets initialize the model. Define the loss function and optimizer. . model = Net().to(device) optimizer = optim.Adam(model.parameters(), lr=lr, betas=(beta1, beta2)) criterion_CE = nn.CrossEntropyLoss().to(device) . Here the .to(device) means to transfer the model and loss function to the specified device. If a CUDA enabled GPU was found on the system then the model and loss will utilize it. . Now lets write the training loop. . epoch = 10 model.train() for e in range(epoch): print(f&#39;Starting epoch {e} of {epoch}&#39;) for X, y in trainloader: X = X.to(device) predictions = model(X) optimizer.zero_grad() loss = criterion_CE(predictions, y) loss.backward() optimizer.step() print(f&#39;Loss: {loss.item()}&#39;) torch.save(model.state_dict(), &quot;model.pt&quot;) . We will train for 10 epochs. Before training we set the model to training mode since behaviour of some layers like Batchnorm and Dropout is different in eval mode and train mode. In this case we are not using these kind of layers but it’s best practice to set the mode. . We iterate over the trainloader which return a batch of images and labels for each iteration. First we transfer the images to specified device then we pass the images to our model and get predictions. Then we zero out our gradients and calculate the loss. After this we calculate our gradient with loss.backward() and finally apply those gradients with optimizer.step(). That’s it we loop over these steps for the whole dataset. . At the end we save the weights of the trained model. The training should take at most 30 minutes without GPU. . Write the Testing Loop . Lets test our model. . model.eval() correct = 0 for X, y in testloader: with torch.no_grad(): X = X.to(device) output = model(X) predictions = output.max(1)[1] correct += torch.eq(predictions, y).sum() print(f&#39;accuracy: {int(correct)}/{len(testloader.dataset)} ({int(correct)/len(testloader.dataset)} or {int(correct)/len(testloader.dataset) * 100}%)&#39;) . The with torch.no_grad(): context tells PyTorch not to calculate gradient of tensor operations within it. Since we don’t need to calculate gradient during testing this increases performance and decreases memory usage. . After getting the predictions from the model we get index of the highest score. The index indicates the digit. We compare the indexes with the ground truth with torch.eq() which returns True for a match and False otherwise. Finally we sum over the resulting tensor. All True&#39;s count as one and False&#39;s as zero. So the sum will be the number of correct predictions. . If you trained for 10 epochs then your accuracy should be about 98%. Which is pretty good. . Full Code . import torch import torch.nn as nn import torch.optim as optim import torchvision import torchvision.datasets as datasets import torchvision.transforms as transforms preprocess = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.5, ), (0.5, )) ]) trainset = datasets.MNIST(&#39;dataset/&#39;, train=True, download=True, transform=preprocess) testset = datasets.MNIST(&#39;dataset/&#39;, train=False, download=True, transform=preprocess) trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True) nf = 32 # conv layer sizes or number of filters/kernels lr = 0.0001 beta1 = 0.5 beta2 = 0.999 device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot; class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.convs = nn.Sequential( nn.Conv2d(1, nf, kernel_size=3, stride=1, padding=1), nn.ReLU(), nn.Conv2d(nf, nf, kernel_size=3, stride=1, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2), # result will be 14x14 nn.Conv2d(nf, nf, kernel_size=3, stride=1, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2), # result will be 7x7 ) self.linears = nn.Sequential( nn.Linear(1568, 100), # width=7, height=7, depth/filters/kernels=32 ; linear_input=7*7*32=1568 nn.ReLU(), nn.Linear(100, 50), nn.ReLU(), nn.Linear(50, 10), ) def forward(self, x): x = self.convs(x) x = x.view(x.size(0), -1) # flattening, result will be (64, 1568) x = self.linears(x) return x model = Net().to(device) optimizer = optim.Adam(model.parameters(), lr=lr, betas=(beta1, beta2)) criterion_CE = nn.CrossEntropyLoss().to(device) model.train() epoch = 10 for e in range(epoch): print(f&#39;Starting epoch {e} of {epoch}&#39;) for X, y in trainloader: X = X.to(device) predictions = model(X) optimizer.zero_grad() loss = criterion_CE(predictions, y) loss.backward() optimizer.step() print(f&#39;Loss: {loss.item()}&#39;) torch.save(model.state_dict(), &quot;model.pt&quot;) model.eval() correct = 0 for X, y in testloader: with torch.no_grad(): X = X.to(device) output = model(X) predictions = output.max(1)[1] correct += torch.eq(predictions, y).sum() print(f&#39;accuracy: {int(correct)}/{len(testloader.dataset)} ({int(correct)/len(testloader.dataset)} or {int(correct)/len(testloader.dataset) * 100}%)&#39;) . I hope you found this helpful. To learn more about PyTorch I highly recommend the official documentation. .",
            "url": "https://hassanaskary.com/python/pytorch/2020/05/20/getting-started-with-pytorch.html",
            "relUrl": "/python/pytorch/2020/05/20/getting-started-with-pytorch.html",
            "date": " • May 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "How to Setup Neovim with Python Provider using Conda",
            "content": "In this post I will go over the steps to setup Neovim with Python provider which is the Python language tool for Neovim. It is required by many Python specific plugins. I’ll be using Conda environments to do this. . Firstly, if you don’t have Neovim installed then install it. On Ubuntu type: . sudo apt install neovim . Neovim requires the pynvim Python package to enable Python support. It can be downloaded from PyPI using pip. If you use Python environments you will have to install pynvim for every environment in which you want to use Neovim. Fortunately, there is a work around, we can create a Python environment and install pynvim in it. Then in our init.vim we will tell Neovim to look for the Python provider in that environment. . I’ll use Conda to create the environments. Of course you’ll need Conda to be installed on your system. To create a Conda environment type: . conda create -n pynvim python=3.7 . I named the environment pynvim and installed Python 3.7 in it. Now activate the environment by typing: . conda activate pynvim . Lets install pynvim. Type: . pip install pynvim . pynvim is installed in pynvim conda environment. We need to know the location of the environment. To do that type: . which python . Note the returned path we will need it. Now open your init.vim and add this line in it: . let g:python3_host_prog=&#39;/path/to/conda/environment&#39; . Put the result of which python inside the single quotes &#39;&#39;. . This is it. You’ve now configured Neovim for Python. You can now go ahead and install plugins. One really cool plugin is Semshi it does semantic syntax highlighting for Python. . I hope this was helpful for you. .",
            "url": "https://hassanaskary.com/vim/conda/python/2020/04/27/how-to-setup-neovim-with-python-provider-using-conda.html",
            "relUrl": "/vim/conda/python/2020/04/27/how-to-setup-neovim-with-python-provider-using-conda.html",
            "date": " • Apr 27, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Studying Computer Science from Pakistan Institute of Engineering and Applied Sciences (PIEAS). Interested in Deep Learning and Computer Vision. Here I write about the new things I learn related to CS/DL/CV. . You can email me at hassan.askary [at] outlook [dot] com or view my resume here. .",
          "url": "https://hassanaskary.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://hassanaskary.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}